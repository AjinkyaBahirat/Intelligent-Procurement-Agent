# --- LLM CONFIGURATION ---
# Provider can be: simple string prefix used by LiteLLM (e.g., 'groq', 'openai', 'vertex_ai')
LLM_PROVIDER=vertex_ai
# Model name. If provider is 'vertex_ai', typical is 'gemini-1.5-pro'. 
# If 'groq', typical is 'llama3-70b-8192'.
LLM_MODEL_NAME=gemini-1.5-pro

# --- EMBEDDING CONFIGURATION ---
# Provider for embeddings (e.g., 'vertex_ai', 'openai')
EMBEDDING_PROVIDER=vertex_ai
# Model name for embeddings (e.g., 'text-embedding-004', 'text-embedding-3-small')
EMBEDDING_MODEL_NAME=text-embedding-004

# --- OLLAMA CONFIGURATION (Alternative for Local Embeddings) ---
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL_NAME=nomic-embed-text
# OLLAMA_API_BASE=http://localhost:11434

# --- API KEYS ---
# Uncomment and set the relevant keys for your providers
# GROQ_API_KEY=gsk_...
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-...

# --- GOOGLE CLOUD (If using Vertex AI) ---
# GOOGLE_CLOUD_PROJECT=your-project-id
# GOOGLE_CLOUD_LOCATION=us-central1

# --- VECTOR STORE ---
VECTOR_DB_PATH=./chroma_db
